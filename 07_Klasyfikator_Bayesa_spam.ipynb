{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNDQCDSFVdQ4"
   },
   "source": [
    "# **SPAM vs. naiwny klasyfikator Bayesa**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seqEdVR1VdQ7"
   },
   "source": [
    "## Wprowadzenie\n",
    "Nigeryjski książę wciąż zarabia na użytkownikach elektronicznych skrzynek pocztowych ponad 700 tys. dolarów rocznie ([źródło](https://www.cnbc.com/2019/04/18/nigerian-prince-scams-still-rake-in-over-700000-dollars-a-year.html))! Jak to możliwe?\n",
    "\n",
    "Pierwsza przyczyna jest natury psychologicznej. Ofiary są poddawane \"perfekcyjnej burzy pokuszeń\", jak ujął to psycholog w wywiadzie, do którego linka dałam Wam powyżej. Spammerzy łączą granie na ludzkiej chciwości, ale także na pragnieniu bycia bohaterem. W końcu kto nie chciałby zarobić na byciu wspaniałomyślnym i szczodrym? W tej kwestii możemy pracować wyłącznie nad sobą.\n",
    "\n",
    "Możemy za to pracować nad filtrami antyspamowymi. Użyjemy techniki, która nazywa się \"worek ze słowami\" (bag of words) w połączeniu z naiwnym klasyfikatorem Bayesa. Choć to prosty klasyfikator, z powodzeniem jest używany współcześnie (np. [SpamAssassin](https://cwiki.apache.org/confluence/display/spamassassin/BayesInSpamAssassin)).\n",
    "\n",
    "Notebook oparty na tutorialach:\n",
    "*   https://towardsdatascience.com/spam-classifier-in-python-from-scratch-27a98ddd8e73\n",
    "*   https://towardsdatascience.com/spam-filtering-using-naive-bayes-98a341224038\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 786,
     "status": "ok",
     "timestamp": 1606307159888,
     "user": {
      "displayName": "Anna Dawid",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgUQAwZ7wyayL4BbiM0n_EANCgBjSdZ9H14lgcCFEE=s64",
      "userId": "02862484648310443813"
     },
     "user_tz": -60
    },
    "id": "cWBDlNPHVdQ9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZJI7KHjVdQ7"
   },
   "source": [
    "## Import danych treningowych\n",
    "https://www.kaggle.com/uciml/sms-spam-collection-dataset\n",
    "\n",
    "To dane przygotowane przez Almeida et al. na podstawie forum brytyjskiego, gdzie użytkownicy skarżą się na spamowe SMSy. Każdy wiersz składa się z kolumny opisującej czy wiadomość jest spamem, czy nie ('spam' czy 'ham'), a druga zawiera treść wiadomości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "executionInfo": {
     "elapsed": 780,
     "status": "ok",
     "timestamp": 1606307159889,
     "user": {
      "displayName": "Anna Dawid",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgUQAwZ7wyayL4BbiM0n_EANCgBjSdZ9H14lgcCFEE=s64",
      "userId": "02862484648310443813"
     },
     "user_tz": -60
    },
    "id": "JHEGrXdx97uU",
    "outputId": "dc391a1b-d56b-4d61-bf88-724358eafe9a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dane/spam.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "054a8tEtDWw2"
   },
   "source": [
    "Dane zawierają zbyteczne kolumny. **Proszę**:\n",
    "* usunać kolumny zawierająca wartości \"NaN\"\n",
    "* zmienić nazwy kolumn \"v1\" i \"v2\" na \"label\", \"text\"\n",
    "\n",
    "**Wskazówka**: proszę użyć metod ```DataFrame.dropna()``` oraz ```DataFrame.rename()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "executionInfo": {
     "elapsed": 1032,
     "status": "ok",
     "timestamp": 1606307160149,
     "user": {
      "displayName": "Anna Dawid",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgUQAwZ7wyayL4BbiM0n_EANCgBjSdZ9H14lgcCFEE=s64",
      "userId": "02862484648310443813"
     },
     "user_tz": -60
    },
    "id": "_e503qPbB-xa",
    "outputId": "505eac09-6b0c-487f-c6be-654804248a5a"
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "df = df.dropna(axis=1)\n",
    "df = df.rename(columns={'v1': 'label', 'v2': 'text'})\n",
    "#END_SOLUTION\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xr86NzYAVdRF"
   },
   "source": [
    "**Proszę**: wypisać na ekran treść maila o indeksie 57\n",
    "\n",
    "**Wskazówka**: Indeksy obiektu DataFreame uzyskujemy przez pole ```DataFrame.index```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1022,
     "status": "ok",
     "timestamp": 1606307160151,
     "user": {
      "displayName": "Anna Dawid",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgUQAwZ7wyayL4BbiM0n_EANCgBjSdZ9H14lgcCFEE=s64",
      "userId": "02862484648310443813"
     },
     "user_tz": -60
    },
    "id": "kBsb284aVdRG"
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "index = 57\n",
    "print(df[df.index==57])\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFpUrBiRgFiV"
   },
   "source": [
    "## Analiza częstości występowania słów w obu klasach za pomocą biblioteki WordCloud\n",
    "\n",
    "To biblioteka pozwalająca generować śliczne obrazki, na których wielkość słów odpowiada częstości jego występowania w danym zbiorze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3564,
     "status": "ok",
     "timestamp": 1606307162716,
     "user": {
      "displayName": "Anna Dawid",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgUQAwZ7wyayL4BbiM0n_EANCgBjSdZ9H14lgcCFEE=s64",
      "userId": "02862484648310443813"
     },
     "user_tz": -60
    },
    "id": "lpfU-WXKg5CY",
    "outputId": "116dff36-1309-4075-ba20-0b69f673c967"
   },
   "outputs": [],
   "source": [
    "!pip3 install wordcloud\n",
    "import wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej kod generujący obrazki dla spamu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 972
    },
    "executionInfo": {
     "elapsed": 7261,
     "status": "ok",
     "timestamp": 1606307166419,
     "user": {
      "displayName": "Anna Dawid",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgUQAwZ7wyayL4BbiM0n_EANCgBjSdZ9H14lgcCFEE=s64",
      "userId": "02862484648310443813"
     },
     "user_tz": -60
    },
    "id": "L28LyPkEivX2",
    "outputId": "03d6e3d5-ca6a-4081-d0af-b8ce397a32f7"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "spam_words = \" \".join(list(df [df['label']=='spam']['text'] ))\n",
    "spam_plot = WordCloud(width = 512, height = 512).generate(spam_words)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(spam_plot);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proszę**: Stwórz analogiczny obrazek dla klasy 'ham'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "ham_words = \" \".join(list(df [df['label']=='ham']['text'] ))\n",
    "ham_plot = WordCloud(width = 512, height = 512).generate(ham_words)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(ham_plot);\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxelOYTFVdRR"
   },
   "source": [
    "Dane w tej chwili są w postaci ciągów słów. Zamienimy je na postać numeryczną używająć algorymtu ```CountVectorizer```.\n",
    "Na początek zróbmy to dla prostego tekstu by zrozumieć jak działa ten algorytm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text = [\"Ala ala ma kota.\", \"Kot? Kot ma wszy.\"]\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(text)\n",
    "print(\"Vocabulary:\", vectorizer.vocabulary_)\n",
    "print(\"Lista słów:\", vectorizer.get_feature_names_out())\n",
    "text_transformed = vectorizer.transform(text)\n",
    "print(\"Original text:\",text)\n",
    "print(\"Transformed text:\",text_transformed)\n",
    "print(\"Transformed text after decoding\",vectorizer.inverse_transform(text_transformed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proszę**:\n",
    "\n",
    "* przeprowadzić procedurę treningu i transformacji dla danych z e-maili.\n",
    "* W dokumentacji CountVectorizer znajdź informację o parametrach max_df i min_df. Następnie spróbuj zastosować te parametry. Wpływ ich działania możesz kontrolować poprzez metodę ```vectorizer.stop_words_```.\n",
    "* wypisać na ekran postać oryginalną i po transformacji maila o indeksie **57**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7254,
     "status": "ok",
     "timestamp": 1606307166421,
     "user": {
      "displayName": "Anna Dawid",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgUQAwZ7wyayL4BbiM0n_EANCgBjSdZ9H14lgcCFEE=s64",
      "userId": "02862484648310443813"
     },
     "user_tz": -60
    },
    "id": "WvM-XCWXVdRV"
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vectorizer = CountVectorizer(max_df=0.1)\n",
    "vectorizer.fit(df.text)\n",
    "data = df.text\n",
    "idx = 57\n",
    "text = df[df.index==57]\n",
    "text_transformed = vectorizer.transform(df.text)\n",
    "print(\"Original text:\",df[df.index==57])\n",
    "print(\"Transformed text:\",text_transformed[df.index==57])\n",
    "print(\"Transformed text after decoding\",vectorizer.inverse_transform(text_transformed[df.index==57]))\n",
    "print(\"Terms that were ignored\",vectorizer.stop_words_)\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHsTCZS6VdRg"
   },
   "source": [
    "## Trening klasyfikatora\n",
    "\n",
    "**Proszę**:\n",
    "* stworzyć kolumnę num_label w której 'spam' będzie oznaczony 1, a 'ham' 0\n",
    "* podzielić dane na część treningową i testową w stosunku **7:3**\n",
    "* wytrenować klasyfikator mail korzysjając z naiwnego algorytmu Bayesa opartego o rozkład wielomianowy: ```MultinomialNB```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7229,
     "status": "ok",
     "timestamp": 1606307166424,
     "user": {
      "displayName": "Anna Dawid",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgUQAwZ7wyayL4BbiM0n_EANCgBjSdZ9H14lgcCFEE=s64",
      "userId": "02862484648310443813"
     },
     "user_tz": -60
    },
    "id": "FyJmhv-VVdRk",
    "outputId": "f7732f13-6533-44bb-cd69-171ebe16ed47"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# stwórz nową kolumnę\n",
    "#BEGIN_SOLUTION\n",
    "df['num_label'] = df.label == 'spam'\n",
    "#END_SOLUTION\n",
    "\n",
    "# Podziel dane\n",
    "#BEGIN_SOLUTION\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_transformed, df.num_label, test_size=0.3)\n",
    "#END_SOLUTION\n",
    "\n",
    "# zaimportuj odpowiednią bibliotekę\n",
    "#BEGIN_SOLUTION\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#END_SOLUTION\n",
    "\n",
    "# stwórz obiekt klasyfikatora\n",
    "#BEGIN_SOLUTION\n",
    "model = MultinomialNB()\n",
    "#END_SOLUTION\n",
    "\n",
    "# naucz klasyfikator na zbiorze uczącym\n",
    "#BEGIN_SOLUTION\n",
    "model.fit(X_train, Y_train);\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kWchjUPVdRn"
   },
   "source": [
    "## Ocena jakości\n",
    "\n",
    "**Proszę**: Spróbuj sam/sama ocenić jakość modelu. Wybierz metryki, które według Ciebie będą najlepiej sprawdzały czy nasz model jest dobrym filtrem spamowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 622,
     "status": "ok",
     "timestamp": 1606307169918,
     "user": {
      "displayName": "Anna Dawid",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgUQAwZ7wyayL4BbiM0n_EANCgBjSdZ9H14lgcCFEE=s64",
      "userId": "02862484648310443813"
     },
     "user_tz": -60
    },
    "id": "ktsCYepybxqu",
    "outputId": "428e624a-e083-4d44-ab08-5614eff0c9a5"
   },
   "outputs": [],
   "source": [
    "# Zaimportuj potrzebne funkcje z sklearn.metrics\n",
    "#BEGIN_SOLUTION\n",
    "from sklearn.metrics import matthews_corrcoef, ConfusionMatrixDisplay\n",
    "#END_SOLUTION\n",
    "\n",
    "def printScores(model, X, Y):\n",
    "    #BEGIN_SOLUTION\n",
    "    y_pred = model.predict(X)\n",
    "    ConfusionMatrixDisplay.from_estimator(model, X, Y)\n",
    "    mcc = matthews_corrcoef(Y, y_pred)\n",
    "    print(mcc)\n",
    "    #END_SOLUTION\n",
    "\n",
    "printScores(model, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXqzqM0cVdRs"
   },
   "source": [
    "## Analiza modelu\n",
    "\n",
    "Sprawdźmy, czego właściwie maszyna się nauczyła. Analizując współczynniki modelu proszę wskazać słowa które są istotne dla klasyfikacji.\n",
    "\n",
    "**Proszę**:\n",
    "* obliczyć prawdopodobieństwa przypisane do kolejnych słów (```np.exp(model.feature_log_prob_)```)\n",
    "* stworzyć listę o długości zawierającą indeksy posortowanych współczynników: ```np.argsort(...)```\n",
    "* wypisać na ekran po N słów o największych wartościach współczynników dla każdej z klas malejąco.\n",
    "\n",
    "**Wskazówka** by listę słów moć adresować listą indeksów, listę słów trzeba zamienić na macierz numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 606,
     "status": "ok",
     "timestamp": 1606307173324,
     "user": {
      "displayName": "Anna Dawid",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgUQAwZ7wyayL4BbiM0n_EANCgBjSdZ9H14lgcCFEE=s64",
      "userId": "02862484648310443813"
     },
     "user_tz": -60
    },
    "id": "Br2t5MLZVdRt",
    "outputId": "44a4bbdc-7e48-4552-ca60-a2ae5f2417dd"
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "# Stwórz macierz numpy 'feature_names' ze słowami zapisanymi w module vectorizer:\n",
    "#BEGIN_SOLUTION\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_names = np.array(feature_names)\n",
    "#END_SOLUTION\n",
    "\n",
    "# Stwórz macierz 'coeff' z prawdopodobieństwami odpowiadającymi słowom z macierzy 'feature_names':\n",
    "#BEGIN_SOLUTION\n",
    "coeff = np.exp(model.feature_log_prob_)\n",
    "#END_SOLUTION\n",
    "\n",
    "# Znajdź indeksy N najważniejszych słów dla każdej z klas\n",
    "#BEGIN_SOLUTION\n",
    "imp_spam = np.argsort(coeff[1])[-N:][::-1]\n",
    "imp_ham = np.argsort(coeff[0])[-N:][::-1]\n",
    "#END_SOLUTION\n",
    "\n",
    "print(\"Słowa, które z największą pewnością wskazują maszynie, że wiadomość to spam:\")\n",
    "print(feature_names[imp_spam])\n",
    "\n",
    "print(\"Słowa, które z największą pewnością wskazują maszynie, że wiadomość to nie spam:\")\n",
    "print(feature_names[imp_ham])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gdybyście byli spammerami... Co moglibyście zrobić, znając tę technikę antyspamową?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stosowanie znaków specjalnych zamiast liter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_message = vectorizer.transform(['call for free'])\n",
    "print(model.predict(our_message))\n",
    "\n",
    "our_tricky_message = vectorizer.transform(['c@ll for free'])\n",
    "print(model.predict(our_tricky_message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jakieś inne pomysły? :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praca domowa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proszę**: Przeprowadź analogiczną analizę dla zbioru tytułów artykułów (https://www.kaggle.com/datasets/algord/fake-news?resource=download) posiadających informację o tym czy dany artykuł jest prawdziwy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytaj dane\n",
    "#BEGIN_SOLUTION\n",
    "df = pd.read_csv('./dane/FakeNewsNet.csv')[['title', 'real']]\n",
    "df = df.rename(columns={'real': 'label', 'title': 'text'})\n",
    "#END_SOLUTION\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narysuj WordCloud dla obu przypadków\n",
    "#BEGIN_SOLUTION\n",
    "real_news_words = \" \".join(list(df [df['label']==1]['text'] ))\n",
    "real_news_plot = WordCloud(width = 512, height = 512).generate(real_news_words)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(real_news_plot);\n",
    "\n",
    "fake_news_words = \" \".join(list(df [df['label']==0]['text'] ))\n",
    "fake_news_plot = WordCloud(width = 512, height = 512).generate(fake_news_words)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(fake_news_plot);\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wytrenuj CountVectorizer i zrób transformację. Dopasuj parametry max_df i min_df.\n",
    "#BEGIN_SOLUTION\n",
    "vectorizer = CountVectorizer(max_df=0.05, min_df=0.0001)\n",
    "vectorizer.fit(df.text)\n",
    "data = df.text\n",
    "idx = 57\n",
    "text = df[df.index==57]\n",
    "text_transformed = vectorizer.transform(df.text)\n",
    "print(\"Original text:\",df[df.index==57].values)\n",
    "print(\"Transformed text:\",text_transformed[df.index==57])\n",
    "print(\"Transformed text after decoding\",vectorizer.inverse_transform(text_transformed[df.index==57]))\n",
    "print(\"Terms that were ignored\",vectorizer.stop_words_)\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podziel dane na zbiór treningowy i testowy\n",
    "#BEGIN_SOLUTION\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_transformed, df.label, test_size=0.3)\n",
    "#END_SOLUTION\n",
    "\n",
    "# stwórz obiekt klasyfikatora i naucz go na zbiorze uczącym\n",
    "#BEGIN_SOLUTION\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, Y_train);\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przeprowadź analizę jakości modelu. Krótko uzasadnij wybór metryk.\n",
    "\n",
    "def printScores(model, X, Y):\n",
    "    #BEGIN_SOLUTION\n",
    "    y_pred = model.predict(X)\n",
    "    ConfusionMatrixDisplay.from_estimator(model, X, Y)\n",
    "    mcc = matthews_corrcoef(Y, y_pred)\n",
    "    print(mcc)\n",
    "    #END_SOLUTION\n",
    "\n",
    "printScores(model, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Znajdź 10 najbardziej istotnych słów dla obu klas\n",
    "#BEGIN_SOLUTION\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_names = np.array(feature_names)\n",
    "n = 10\n",
    "coeff = np.exp(model.feature_log_prob_)\n",
    "imp_spam = np.argsort(coeff[1])[-n:][::-1]\n",
    "imp_ham = np.argsort(coeff[0])[-n:][::-1]\n",
    "\n",
    "print(\"Słowa, które z największą pewnością wskazują maszynie, że artykuł jest prawdziwy:\")\n",
    "print(feature_names[imp_spam])\n",
    "\n",
    "print(\"Słowa, które z największą pewnością wskazują maszynie, że artykuł nie jest prawdziwy:\")\n",
    "print(feature_names[imp_ham])\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "05M_Bayes_spam.ipynb",
   "provenance": [
    {
     "file_id": "1BY9TSukNLV3VgtNdHpcgWofeAtVF4yku",
     "timestamp": 1573037493591
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
